# Transformer Implementation
A Pytorch Implementation of the paper "Attention is All You Need".   
I checked out several popular implementations and I have found a few points which was quite different from the original paper.   
This repository is the result of fixing errors and cleaning codes in pytorch-OOP manner. 

## References
- [Attention Is All You Need, Vaswani et al.](https://arxiv.org/abs/1706.03762)
- [The Annotated Transformer, Harvard NLP.](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention is all you need: A Pytorch Implementation, Yu-Hsiang Huang](https://github.com/jadore801120/attention-is-all-you-need-pytorch)
- [A Transformer Implementation of Attention is All You Need, Kyubyoung Park](https://github.com/Kyubyong/transformer)
- [Transformers, Huggingface](https://github.com/huggingface/transformers)

## Author
- This repository is developed and maintained by Yonghee Cheon (yonghee.cheon@gmail.com).      
- It can be found here: https://github.com/yonghee12/transformer_torch
- Linkedin Profile: https://www.linkedin.com/in/yonghee-cheon-7b90b116a/